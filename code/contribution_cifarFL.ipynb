{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvuWze6kjxhV"
      },
      "source": [
        "## Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UihnNmjPjxhV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "\n",
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -10}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -10}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca pi√π alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_skewed_array(target_sum, num_elements, skew_factor=0):\n",
        "    \"\"\"\n",
        "    Generate an array of integers that sum up to a target value.\n",
        "\n",
        "    Args:\n",
        "        target_sum (int): The target sum of the array.\n",
        "        num_elements (int): The number of elements in the array.\n",
        "        skew_factor (float): A positive skew factor favors smaller numbers,\n",
        "                             a negative skew factor favors larger numbers,\n",
        "                             and 0 results in an approximately uniform distribution.\n",
        "\n",
        "    Returns:\n",
        "        list: An array of integers summing up to the target value.\n",
        "    \"\"\"\n",
        "    if num_elements <= 0:\n",
        "        raise ValueError(\"num_elements must be greater than 0\")\n",
        "\n",
        "    # Generate a distribution using Dirichlet\n",
        "    alpha = [1 + skew_factor] * num_elements if skew_factor >= 0 else [1 - skew_factor] * num_elements\n",
        "    raw_weights = np.random.dirichlet(alpha)\n",
        "\n",
        "    # Scale weights to the target sum\n",
        "    scaled_weights = raw_weights * target_sum\n",
        "\n",
        "    # Convert to integers while keeping track of the fractional parts\n",
        "    integer_parts = np.floor(scaled_weights).astype(int)\n",
        "    fractional_parts = scaled_weights - integer_parts\n",
        "\n",
        "    # Adjust the result to ensure the sum matches the target_sum\n",
        "    diff = target_sum - np.sum(integer_parts)\n",
        "\n",
        "    # Distribute the difference based on the largest fractional parts\n",
        "    fractional_indices = np.argsort(-fractional_parts)\n",
        "    for i in range(abs(diff)):\n",
        "        integer_parts[fractional_indices[i]] += 1 if diff > 0 else -1\n",
        "\n",
        "    return integer_parts.tolist()\n",
        "\n",
        "def generate_vector(target_sum, n):\n",
        "    \"\"\"\n",
        "    Generates a vector (list) with n positive elements that sum to target_sum.\n",
        "    Used for niid sharding\n",
        "\n",
        "    Parameters:\n",
        "    - target_sum: The desired sum of the vector elements.\n",
        "    - n: The number of elements in the vector.\n",
        "\n",
        "    Returns:\n",
        "    - A list of n positive elements that sum to target_sum.\n",
        "    \"\"\"\n",
        "    if target_sum < n:\n",
        "        raise ValueError(\"Target sum must be at least equal to the number of elements to ensure all elements are positive.\")\n",
        "\n",
        "    # Create n random weights that sum to 1\n",
        "    weights = [random.random() for _ in range(n)]\n",
        "    total_weight = sum(weights)\n",
        "    normalized_weights = [w / total_weight for w in weights]\n",
        "\n",
        "    # Scale the weights to sum to the target_sum\n",
        "    vector = [max(1, int(target_sum * w)) for w in normalized_weights]\n",
        "\n",
        "    # Adjust to ensure the exact sum of target_sum\n",
        "    current_sum = sum(vector)\n",
        "    while current_sum != target_sum:\n",
        "        # Find the difference between the current sum and the target sum\n",
        "        difference = target_sum - current_sum\n",
        "\n",
        "        # Adjust a random element to fix the difference\n",
        "        index_to_adjust = random.randint(0, n - 1)\n",
        "        if difference > 0:\n",
        "            vector[index_to_adjust] += 1\n",
        "        elif vector[index_to_adjust] > 1:  # Ensure no element goes below 1\n",
        "            vector[index_to_adjust] -= 1\n",
        "\n",
        "        # Recalculate the sum\n",
        "        current_sum = sum(vector)\n",
        "\n",
        "    return vector\n",
        "\n"
      ],
      "metadata": {
        "id": "BdzLUdjViUN5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqh7noxfUWv"
      },
      "source": [
        "# CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "outputs": [],
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2, skewed_factor = None):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            if self.split == 'train':\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.RandomHorizontalFlip(),  # Flip orizzontale casuale\n",
        "                    transforms.RandomRotation(10),\n",
        "                    transforms.ToTensor(),  # Converte l'immagine in un tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "            else:\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),  # Converte in tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "      \"\"\"Non-IID sharding with fixed number of classes per client\"\"\"\n",
        "      data_split = []\n",
        "      labels = self.data['label'].unique()\n",
        "      samples_per_client = generate_skewed_array(len(self.data), self.K)\n",
        "\n",
        "      for client_id in range(self.K):\n",
        "          # Seleziona Nc classi casuali per questo client\n",
        "          client_classes = np.random.choice(labels, size=self.Nc, replace=False)\n",
        "\n",
        "          # Ottieni il numero totale di campioni per il client\n",
        "          total_samples = samples_per_client[client_id]\n",
        "\n",
        "          # Assicurati che il numero totale di campioni sia sufficiente per il numero di classi\n",
        "          min_samples_per_class = 5  # Numero minimo di campioni per classe\n",
        "          total_samples = max(total_samples, min_samples_per_class * len(client_classes))\n",
        "\n",
        "          # Distribuisci i campioni tra le classi\n",
        "          samples_per_class = generate_vector(total_samples, len(client_classes))\n",
        "\n",
        "          client_data = pd.DataFrame()\n",
        "          for idx, class_ in enumerate(client_classes):\n",
        "              class_data = self.data[self.data['label'] == class_]\n",
        "              samples = class_data.sample(n=samples_per_class[idx], replace=True)  # replace=False per evitare duplicati\n",
        "              client_data = pd.concat([client_data, samples])\n",
        "\n",
        "          client_data['client_id'] = client_id\n",
        "          data_split.append(client_data)\n",
        "\n",
        "      # Concatenate tutti i dati per avere il dataset completo per tutti i clienti\n",
        "      all_data = pd.concat(data_split, ignore_index=True)\n",
        "      return all_data\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptw5_Q9Yv4u"
      },
      "source": [
        "## LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iekAEyoGYx0p"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self,num_classes=100):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 5 * 5, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, num_classes)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, validation_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(validation_loader), correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCdLhW1Vv80"
      },
      "source": [
        "## FL Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "outputs": [],
      "source": [
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client √® stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"CIFAR100_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "\n",
        "    steps = 0\n",
        "    while steps < local_steps:\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps >= local_steps:\n",
        "          break\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data, val_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "\n",
        "  def client_selection(self, num_clients, fraction, probabilities=None):\n",
        "      \"It selects a subset of clients based on uniform or skewed distribution\"\n",
        "      num_clients_to_select = int(num_clients * fraction)\n",
        "      if probabilities is None:\n",
        "          selected_clients = np.random.choice(num_clients, num_clients_to_select, replace=False)\n",
        "      else:\n",
        "          selected_clients = np.random.choice(num_clients, num_clients_to_select, p=probabilities, replace=False)\n",
        "      return selected_clients\n",
        "\n",
        "  def evaluate_clients(self, alpha):\n",
        "    \"\"\"\n",
        "    Evaluate clients using an entropy-based approach.\n",
        "    Clients with more diverse data distributions are prioritized.\n",
        "    Returns:\n",
        "        Probabilities favoring clients with diverse data and larger datasets.\n",
        "    \"\"\"\n",
        "    # Assumendo che client.data sia un elenco di tuple (immagine, etichetta)\n",
        "    #conto numero occorrenze di ogni etichetta\n",
        "    data_distributions = [\n",
        "        np.bincount([label for _, label in client.data], minlength=100) for client in self.clients\n",
        "    ]\n",
        "    entropies = [-np.sum(d / np.sum(d) * np.log(d / np.sum(d) + 1e-5)) for d in data_distributions]\n",
        "\n",
        "    # Normalize entropies\n",
        "    normalized_entropies = np.array(entropies) / np.sum(entropies)\n",
        "\n",
        "    # Combine with data sizes\n",
        "    data_sizes = [len(client.data) for client in self.clients]\n",
        "    normalized_data_sizes = np.array(data_sizes) / np.sum(data_sizes)\n",
        "\n",
        "    # Combine metrics\n",
        "    combined_scores = alpha * normalized_entropies + (1 - alpha) * normalized_data_sizes\n",
        "\n",
        "    # Normalize combined scores to get probabilities\n",
        "    probabilities = combined_scores / np.sum(combined_scores)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, alpha = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    probabilities = None\n",
        "\n",
        "    start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "    probabilities=self.evaluate_clients(alpha) # Generate data/entropy prob\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      selected_clients = self.client_selection(len(self.clients), fraction_fit, probabilities)\n",
        "\n",
        "      self.selected_clients_per_round.append([self.clients[client_idx].client_id for client_idx in selected_clients])\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client_idx in selected_clients:\n",
        "        client = self.clients[client_idx]  # Accedi all'oggetto Client usando l'indice\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(self.clients[client_idx].data) for client_idx in selected_clients])\n",
        "      for client_idx in selected_clients:\n",
        "          client = self.clients[client_idx]  # Accedi all'oggetto Client\n",
        "          scaling_factor = len(client.data) / total_data_size\n",
        "          for key in new_global_weights.keys():\n",
        "              new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model every 10 rounds\n",
        "      if round % 10 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.val_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "        loss_test, accuracy_test = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round],  # Serializziamo solo i client_id\n",
        "        }\n",
        "\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Evaluation on test set...\")\n",
        "    loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     # Plot dei risultati\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Validation Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_losses, label='Validation Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss per Round')\n",
        "    plt.legend()\n",
        "\n",
        "        # Validation Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy per Round')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    file_name = f\"CIFAR100_fedavg_uniform_{hyperparameters}.jpg\"\n",
        "    plt.savefig(file_name)\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the contrib experiments\n",
        "Hyperparameters can be changed here. RUn this cell to start experiments."
      ],
      "metadata": {
        "id": "o0qj6xNx07MQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEh36xySq9Pr"
      },
      "outputs": [],
      "source": [
        "K = 100 #fix\n",
        "LOCAL_STEPS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1 #fix\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "MOMENTUM = 0\n",
        "WEIGHT_DECAY = 1e-4\n",
        "ALPHA=0.5\n",
        "\n",
        "optimizer_params = {\n",
        "      \"lr\": LR,\n",
        "      \"momentum\": MOMENTUM,\n",
        "      \"weight_decay\": WEIGHT_DECAY\n",
        "  }\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "train_dataset_big = CIFAR100Dataset(DIR_DATA, split=\"train\", sharding=\"niid\", K=K, Nc=5)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split=\"test\")\n",
        "\n",
        "# Split training-validation\n",
        "train_indices, validation_indices = train_test_split(\n",
        "    range(len(train_dataset_big)), test_size=0.2, random_state=42,stratify=train_dataset_big.data[\"label\"]\n",
        ")\n",
        "\n",
        "# Subset di training e validazione\n",
        "train_dataset = Subset(train_dataset_big, train_indices)\n",
        "validation_dataset = Subset(train_dataset_big, validation_indices)\n",
        "\n",
        "# Mapping degli indici\n",
        "original_to_subset = {original_idx: subset_idx for subset_idx, original_idx in enumerate(train_indices)}\n",
        "\n",
        "# Creazione dei client\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    # Filtra gli indici dei client dal dataset originale\n",
        "    client_original_indices = train_dataset_big.data[\n",
        "        train_dataset_big.data[\"client_id\"] == i\n",
        "    ].index\n",
        "\n",
        "    # Converte gli indici originali in indici del subset\n",
        "    client_subset_indices = [original_to_subset[idx] for idx in client_original_indices if idx in original_to_subset]\n",
        "\n",
        "    # Crea il subset per il client\n",
        "    client_data = Subset(train_dataset, client_subset_indices)\n",
        "    clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "\n",
        "\n",
        "server_uniform = Server(model_cifar, clients, test_dataset, validation_dataset)\n",
        "hyperparameters = f\"ENTROPY_CONTRIB_BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_STEPS}_C{C}_ALPHA{ALPHA}\"\n",
        "server_uniform.federated_averaging(local_steps=LOCAL_STEPS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, alpha=ALPHA, fraction_fit=C,hyperparameters=hyperparameters)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}